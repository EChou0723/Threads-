import requests
import os
import time
import random
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
# Telegram è¨­å®šï¼Œå·²ç”±åŠ©ç†ä»£å…¥ä½ çš„è³‡è¨Š
BOT_TOKEN = 'è¼¸å…¥ä½ çš„ Token '
CHAT_ID = 'è¼¸å…¥ä½ çš„ CHAY_ID'
def send_telegram(msg):
    """ç™¼é€è¨Šæ¯åˆ° Telegram"""
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    params = {"chat_id": CHAT_ID, "text": msg}
    requests.get(url, params=params)
def extract_post_and_replies(driver, url, username):
    """å¾ Threads è²¼æ–‡æŠ“å–ä¸»è¦å…§å®¹èˆ‡ç•™è¨€ï¼Œè¿”å›å­—ä¸²"""
    driver.get(url)
    time.sleep(random.uniform(8, 15))  # éš¨æ©Ÿç­‰å¾…é˜²åçˆ¬
    items = []
    try:
        if "ç™»å…¥" in driver.page_source or "Login" in driver.page_source:
            print(f"[ERROR] éœ€é‡æ–°ç™»å…¥: {url}")
            return "LOGIN_REQUIRED"
        blocks = driver.find_elements(By.XPATH, '//div[@role="region"]') or driver.find_elements(By.XPATH, '//article')
        print(f"è§£æè²¼æ–‡å€å¡Šæ•¸: {len(blocks)} in {url}")
        for block in blocks:
            try:
                spans = block.find_elements(By.XPATH, ".//span[@dir='auto']")
                for s in spans:
                    txt = s.text.strip()
                    if re.search(r'[ã€‚ï¼ï¼Ÿï¼Œ,.A-Za-z0-9]', txt) and len(txt) > 3:
                        items.append(txt)
            except Exception:
                continue
        if len(items) == 0:
            print(f"[WARN] æ²’æŠ“åˆ°ä¸»æ–‡/ç•™è¨€: {url}")
            return "NO_TEXT_FOUND"
    except Exception as e:
        print(f"[ERROR] Parse error: {e} url: {url}")
        return f"FETCH_ERROR: {e}"
    return '\n'.join(list(dict.fromkeys(items)))
def detect_empty_posts(input_csv, output_csv):
    df = pd.read_csv(input_csv)
    error_conditions = ["", "LOGIN_REQUIRED", "NO_TEXT_FOUND", "nan"]
    urls = df[df["post_text_and_replies"].astype(str).str.strip().isin(error_conditions)]["url"].tolist()
    if urls:
        pd.Series(urls).to_csv(output_csv, index=False)
        print(f"éœ€è£œæŠ“ç¶²å€å…± {len(urls)} ç­†ï¼Œå·²è¼¸å‡º {output_csv}")
    else:
        print("æ‰€æœ‰å…§å®¹éƒ½å·²æ­£å¸¸æŠ“å–å®Œæˆï¼")
    return len(urls)
if __name__ == "__main__":
    # è«‹å°‡ä¸‹æ–¹è·¯å¾‘ä¿®æ”¹æˆä½ å¯¦éš› iCloud æ¡Œé¢çš„è³‡æ–™å¤¾è·¯å¾‘
    base_dir = "/Users/yourbaby/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Python çˆ¬ Threads è³‡æ–™"
    
    finished_file = os.path.join(base_dir, "make_investment_easy_threads_main_posts.csv")
    refetch_file = os.path.join(base_dir, "threads_urls_to_refetch.csv")
    all_url_file = os.path.join(base_dir, "all_threads_urls_first_crawl.csv")
    
    username = "make_investment_easy"
    
    options = Options()
    options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")
    options.add_argument("accept-language=zh-TW,zh;q=0.9,en;q=0.8")
    
    if os.path.exists(finished_file):
        empty_count = detect_empty_posts(finished_file, refetch_file)
        if empty_count > 0:
            url_list = pd.read_csv(refetch_file).iloc[:, 0].tolist()
            print(f"ã€æœ¬æ¬¡å„ªå…ˆè£œæŠ“ {len(url_list)} ç¯‡ã€‘")
        else:
            print("ã€æ²’æœ‰éœ€è¦è£œæŠ“çš„å…§å®¹ï¼Œå°‡é€²è¡Œå…¨é‡æª¢æŸ¥ã€‘")
            url_list = pd.read_csv(all_url_file).iloc[:, 0].tolist()
    else:
        url_list = pd.read_csv(all_url_file).iloc[:, 0].tolist()
        print(f"ã€é¦–æ¬¡æŠ“å–ï¼Œå…± {len(url_list)} ç¯‡ã€‘")
    
    already_done = set()
    if os.path.exists(finished_file):
        try:
            df_exist = pd.read_csv(finished_file)
            normal_posts = df_exist[~df_exist["post_text_and_replies"].astype(str).str.strip().isin(["LOGIN_REQUIRED", "NO_TEXT_FOUND", "", "nan"])]
            already_done.update(normal_posts["url"].tolist())
        except Exception as e:
            print(f"è®€å–æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    pending_list = [u for u in url_list if u not in already_done]
    print(f"å¾…æŠ“å–å…±ï¼š{len(pending_list)} ç¯‡")
    
    if len(pending_list) == 0:
        print("æ‰€æœ‰å…§å®¹éƒ½å·²å®ŒæˆæŠ“å–ï¼")
        send_telegram("ğŸ‰ æ‰€æœ‰å…§å®¹éƒ½å·²æˆåŠŸæŠ“å–å®Œæˆï¼")
        exit()
    
    driver = webdriver.Chrome(options=options)
    results = []
    try:
        driver.get("https://www.threads.com/")
        input("è«‹åœ¨è‡ªå‹•é–‹å•Ÿçš„ç€è¦½å™¨å…§å®Œæˆç™»å…¥ Threadsï¼Œç¢ºèªèƒ½çœ‹åˆ°å€‹äººä¸»é å¾ŒæŒ‰ Enter ...")
    
        for idx, url in enumerate(pending_list):
            print(f"[{idx+1}/{len(pending_list)}] æŠ“å–: {url}")
            try:
                post_text = extract_post_and_replies(driver, url, username)
                results.append({"url": url, "post_text_and_replies": post_text})
                if "LOGIN_REQUIRED" in post_text:
                    print("æª¢æ¸¬åˆ°ç™»å…¥å¤±æ•ˆï¼Œè«‹é‡æ–°ç™»å…¥...")
                    input("è«‹åœ¨ç€è¦½å™¨é‡æ–°ç™»å…¥å¾ŒæŒ‰ Enter ç¹¼çºŒ...")
            except Exception as e:
                print(f"[ERROR] {url}: {e}")
                results.append({"url": url, "post_text_and_replies": f"FETCH_ERROR: {e}"})
            wait_time = random.uniform(10, 18)
            print(f"ç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)
            if (idx + 1) % 20 == 0:
                print("å·²æŠ“ 20 ç¯‡ï¼Œä¼‘æ¯ 2 åˆ†é˜...")
                time.sleep(120)
    finally:
        driver.quit()
    
    df_new = pd.DataFrame(results)
    if os.path.exists(finished_file):
        try:
            df_exist = pd.read_csv(finished_file)
            df_exist_clean = df_exist[~df_exist["url"].isin([r["url"] for r in results])]
            df_final = pd.concat([df_exist_clean, df_new]).drop_duplicates("url", keep="last")
        except Exception as e:
            print(f"åˆä½µå¤±æ•—: {e}")
            df_final = df_new
    else:
        df_final = df_new
    df_final.to_csv(finished_file, index=False, encoding="utf-8-sig")
    print(f"\nå®Œæˆï¼å…±è™•ç† {len(results)} ç¯‡ï¼Œå·²å­˜æª”è‡³ {finished_file}")
    
    final_empty = detect_empty_posts(finished_file, refetch_file)
    
    new_posts_count = len([r for r in results if r["url"] not in already_done])
    has_crawl_error = any("FETCH_ERROR" in str(r["post_text_and_replies"]) for r in results)
    has_login_error = any("LOGIN_REQUIRED" in str(r["post_text_and_replies"]) for r in results)
    has_other_error = final_empty > 0
    
    if new_posts_count > 0:
        for r in results:
            if r["url"] not in already_done:
                msg = f"ç™½è©±æŠ•è³‡æœ‰æ–°è²¼æ–‡ï¼\nç¶²å€ï¼š{r['url']}\nå…§å®¹ï¼š{r['post_text_and_replies'][:300]}..."
                send_telegram(msg)
        print(f"ç™½è©±æŠ•è³‡æœ‰æ–°è²¼æ–‡ï¼š{new_posts_count} ç¯‡ï¼")
    if has_crawl_error:
        msg = "âš ï¸ çˆ¬èŸ²åŸ·è¡Œéç¨‹ç™¼ç”ŸæŠ“å–éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ logï¼"
        send_telegram(msg)
        print(msg)
    if has_login_error:
        msg = "âš ï¸ ç™»å…¥å¤±æ•ˆéœ€æ‰‹å‹•è£œç™»ï¼Œè«‹æª¢æŸ¥ä¸»æ§å°ï¼"
        send_telegram(msg)
        print(msg)
    if has_other_error:
        msg = f"âš ï¸ å°šæœ‰ {final_empty} ç¯‡å…§å®¹éœ€è£œæŠ“ï¼ˆè«‹æŸ¥çœ‹ refetch æ¸…å–®ï¼‰ï¼"
        send_telegram(msg)
        print(msg)
    if final_empty == 0 and not has_crawl_error and not has_login_error:
        send_telegram("ğŸ‰ æ‰€æœ‰å…§å®¹éƒ½å·²æˆåŠŸæŠ“å–å®Œæˆï¼")
    print("\nã€æœ¬æ¬¡åŸ·è¡Œå®Œç•¢ã€‘")
